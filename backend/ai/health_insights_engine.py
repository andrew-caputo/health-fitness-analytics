"""
Health Insights Engine - Core AI Analytics for Multi-Source Health Data

This module provides AI-powered health insights by analyzing data from multiple sources
including HealthKit, OAuth2 integrations, and file uploads. It generates personalized
recommendations, identifies patterns, and provides actionable health insights.
"""

import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
from enum import Enum
import logging
from sqlalchemy.orm import Session
from sqlalchemy import and_, or_, func

from backend.core.database import get_db
from backend.core.models import HealthMetricUnified, User
from .correlation_analyzer import CorrelationAnalyzer
from .pattern_recognition import PatternRecognizer
from .anomaly_detector import AnomalyDetector
from .recommendation_engine import RecommendationEngine

logger = logging.getLogger(__name__)


def clean_numpy_data(data: Any) -> Any:
    """
    Recursively clean numpy data types from nested dictionaries and lists
    to ensure JSON serialization compatibility
    """
    if isinstance(data, dict):
        return {key: clean_numpy_data(value) for key, value in data.items()}
    elif isinstance(data, list):
        return [clean_numpy_data(item) for item in data]
    elif isinstance(data, np.integer):
        return int(data)
    elif isinstance(data, np.floating):
        return float(data)
    elif isinstance(data, np.ndarray):
        return data.tolist()
    elif isinstance(data, (np.bool_, bool)):
        return bool(data)
    else:
        return data


class InsightType(Enum):
    """Types of health insights that can be generated"""
    CORRELATION = "correlation"
    TREND = "trend"
    ANOMALY = "anomaly"
    RECOMMENDATION = "recommendation"
    ACHIEVEMENT = "achievement"
    WARNING = "warning"
    PATTERN = "pattern"


class InsightPriority(Enum):
    """Priority levels for health insights"""
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"


@dataclass
class HealthInsight:
    """Represents a health insight generated by the AI engine"""
    id: str
    user_id: int
    insight_type: InsightType
    priority: InsightPriority
    title: str
    description: str
    data_sources: List[str]
    metrics_involved: List[str]
    confidence_score: float
    actionable_recommendations: List[str]
    supporting_data: Dict[str, Any]
    created_at: datetime
    expires_at: Optional[datetime] = None


@dataclass
class HealthScore:
    """Overall health score with component breakdowns"""
    overall_score: float
    activity_score: float
    sleep_score: float
    nutrition_score: float
    heart_health_score: float
    consistency_score: float
    trend_score: float
    last_updated: datetime


class HealthInsightsEngine:
    """
    Core AI engine for generating health insights from multi-source data
    """
    
    def __init__(self):
        self.correlation_analyzer = CorrelationAnalyzer()
        self.pattern_recognizer = PatternRecognizer()
        self.anomaly_detector = AnomalyDetector()
        self.recommendation_engine = RecommendationEngine()
        
    def generate_comprehensive_insights(
        self, 
        user_id: int, 
        days_back: int = 30,
        db: Session = None
    ) -> List[HealthInsight]:
        """
        Generate comprehensive health insights for a user
        
        Args:
            user_id: User ID to generate insights for
            days_back: Number of days of historical data to analyze
            db: Database session
            
        Returns:
            List of health insights ordered by priority
        """
        if db is None:
            db = next(get_db())
            
        try:
            # Get user's health data
            health_data = self._get_user_health_data(user_id, days_back, db)
            
            if health_data.empty:
                logger.warning(f"No health data found for user {user_id}")
                return []
            
            insights = []
            
            # Generate different types of insights
            insights.extend(self._generate_correlation_insights(user_id, health_data))
            insights.extend(self._generate_trend_insights(user_id, health_data))
            insights.extend(self._generate_anomaly_insights(user_id, health_data))
            insights.extend(self._generate_pattern_insights(user_id, health_data))
            insights.extend(self._generate_achievement_insights(user_id, health_data))
            insights.extend(self._generate_recommendation_insights(user_id, health_data))
            
            # Sort by priority and confidence
            insights.sort(key=lambda x: (x.priority.value, -x.confidence_score))
            
            logger.info(f"Generated {len(insights)} insights for user {user_id}")
            return insights
            
        except Exception as e:
            logger.error(f"Error generating insights for user {user_id}: {str(e)}")
            return []
    
    def calculate_health_score(
        self, 
        user_id: int, 
        days_back: int = 30,
        db: Session = None
    ) -> Optional[HealthScore]:
        """
        Calculate comprehensive health score with dynamic metric inclusion
        Only includes metrics with available data and prompts users for missing data sources
        
        Args:
            user_id: User ID to calculate score for
            days_back: Number of days of data to include
            db: Database session
            
        Returns:
            HealthScore object or None if insufficient data
        """
        if db is None:
            db = next(get_db())
            
        try:
            # Always try to get data for the EXACT requested period first
            health_data = self._get_user_health_data(user_id, days_back, db)
            actual_days_used = days_back
            
            # Check if we have ANY meaningful data for the requested period
            if not health_data.empty and len(health_data) >= 3:  # Lowered threshold
                unique_metrics = health_data['metric_type'].nunique()
                unique_days = health_data['recorded_at'].dt.date.nunique()
                
                # Use requested period if we have at least some data
                if unique_metrics >= 1 and unique_days >= 2:  # Much more permissive
                    logger.info(f"Using EXACT requested {days_back} days with {len(health_data)} records, {unique_metrics} metrics, {unique_days} days")
                    # Keep the requested period to ensure time-period sensitivity
                else:
                    logger.info(f"Minimal data for {days_back} days, using intelligent fallback")
                    health_data, actual_days_used = self._get_optimal_data_range(user_id, days_back, db)
            else:
                logger.info(f"No data for {days_back} days, using intelligent fallback")
                health_data, actual_days_used = self._get_optimal_data_range(user_id, days_back, db)
                
            if health_data.empty:
                logger.warning(f"No health data found for user {user_id} even with fallback")
                return None
            
            # Identify available metrics and calculate scores ONLY for available data
            available_metrics = self._identify_available_metrics(health_data)
            logger.info(f"Available metrics for scoring: {list(available_metrics.keys())}")
            
            component_scores = {}
            missing_metrics = []
            
            # Activity Score
            if available_metrics.get('activity', False):
                component_scores['activity'] = self._calculate_activity_score(health_data, days_back)
            else:
                missing_metrics.append('activity')
            
            # Sleep Score  
            if available_metrics.get('sleep', False):
                component_scores['sleep'] = self._calculate_sleep_score(health_data, days_back)
            else:
                missing_metrics.append('sleep')
            
            # Nutrition Score
            if available_metrics.get('nutrition', False):
                component_scores['nutrition'] = self._calculate_nutrition_score(health_data, days_back)
            else:
                missing_metrics.append('nutrition')
            
            # Heart Health Score
            if available_metrics.get('heart_health', False):
                component_scores['heart_health'] = self._calculate_heart_health_score(health_data, days_back)
            else:
                missing_metrics.append('heart_health')
            
            # Consistency Score (always available if we have any data)
            component_scores['consistency'] = self._calculate_consistency_score(health_data, days_back)
            
            # Trend Score (always available if we have any data)
            component_scores['trend'] = self._calculate_trend_score(health_data, days_back)
            
            # Log missing metrics for user prompting
            if missing_metrics:
                logger.info(f"Missing metrics (will prompt user): {missing_metrics}")
            
            # Ensure all component scores are valid numbers
            for key, score in component_scores.items():
                if pd.isna(score) or not np.isfinite(score):
                    component_scores[key] = 50.0
            
            # Calculate weighted overall score ONLY from available metrics
            # Dynamic weight redistribution based on available metrics
            base_weights = {
                'activity': 0.25,
                'sleep': 0.20,
                'nutrition': 0.20,
                'heart_health': 0.15,
                'consistency': 0.10,
                'trend': 0.10
            }
            
            # Calculate total weight from available metrics
            available_weight = sum(base_weights[metric] for metric in component_scores.keys())
            
            # Redistribute weights proportionally
            adjusted_weights = {}
            for metric in component_scores.keys():
                adjusted_weights[metric] = base_weights[metric] / available_weight
            
            # Calculate overall score from available metrics only
            overall_score = sum(
                component_scores[metric] * adjusted_weights[metric] 
                for metric in component_scores.keys()
            )
            
            # Final NaN check for overall score
            if pd.isna(overall_score) or not np.isfinite(overall_score):
                overall_score = 50.0
            
            # Fill missing component scores with None to indicate unavailable
            final_component_scores = {
                'activity': component_scores.get('activity', None),
                'sleep': component_scores.get('sleep', None),
                'nutrition': component_scores.get('nutrition', None),
                'heart_health': component_scores.get('heart_health', None),
                'consistency': component_scores.get('consistency', None),
                'trend': component_scores.get('trend', None)
            }
            
            return HealthScore(
                overall_score=round(float(overall_score), 1),
                activity_score=round(float(final_component_scores['activity']), 1) if final_component_scores['activity'] is not None else None,
                sleep_score=round(float(final_component_scores['sleep']), 1) if final_component_scores['sleep'] is not None else None,
                nutrition_score=round(float(final_component_scores['nutrition']), 1) if final_component_scores['nutrition'] is not None else None,
                heart_health_score=round(float(final_component_scores['heart_health']), 1) if final_component_scores['heart_health'] is not None else None,
                consistency_score=round(float(final_component_scores['consistency']), 1) if final_component_scores['consistency'] is not None else None,
                trend_score=round(float(final_component_scores['trend']), 1) if final_component_scores['trend'] is not None else None,
                last_updated=datetime.utcnow()
            )
            
        except Exception as e:
            logger.error(f"Error calculating health score for user {user_id}: {str(e)}")
            return None
    
    def _get_user_health_data(
        self, 
        user_id: int, 
        days_back: int, 
        db: Session
    ) -> pd.DataFrame:
        """Get user's health data as pandas DataFrame"""
        end_date = datetime.utcnow()
        start_date = end_date - timedelta(days=days_back)
        
        # Query health metrics
        metrics = db.query(HealthMetricUnified).filter(
            and_(
                HealthMetricUnified.user_id == user_id,
                HealthMetricUnified.timestamp >= start_date,
                HealthMetricUnified.timestamp <= end_date
            )
        ).all()
        
        if not metrics:
            return pd.DataFrame()
        
        # Convert to DataFrame
        data = []
        for metric in metrics:
            data.append({
                'metric_type': metric.metric_type,
                'value': float(metric.value),  # Convert Decimal to float for proper pandas/numpy processing
                'unit': metric.unit,
                'source_type': metric.data_source,
                'recorded_at': metric.timestamp,
                'metadata': metric.source_specific_data or {}
            })
        
        df = pd.DataFrame(data)
        
        # Ensure numeric columns are properly typed
        if not df.empty:
            df['value'] = pd.to_numeric(df['value'], errors='coerce')
        
        return df
    
    def _get_optimal_data_range(
        self, 
        user_id: int, 
        requested_days: int, 
        db: Session
    ) -> Tuple[pd.DataFrame, int]:
        """
        Get optimal data range by trying progressively larger time periods
        
        Args:
            user_id: User ID
            requested_days: Originally requested days
            db: Database session
            
        Returns:
            Tuple of (DataFrame, actual_days_used)
        """
        try:
            # Try progressively larger time periods: 7, 14, 30, 60, 90, 180, 365
            fallback_periods = [7, 14, 30, 60, 90, 180, 365]
            
            # Start from the requested period or the next larger one
            periods_to_try = [p for p in fallback_periods if p >= requested_days]
            if not periods_to_try:
                periods_to_try = [365]  # Fallback to maximum
            
            for days in periods_to_try:
                logger.info(f"Trying {days} days of data for user {user_id}")
                health_data = self._get_user_health_data(user_id, days, db)
                
                # Check if we have sufficient data diversity
                if not health_data.empty and len(health_data) >= 5:
                    unique_metrics = health_data['metric_type'].nunique()
                    unique_days = health_data['recorded_at'].dt.date.nunique()
                    
                    # Need at least 2 different metrics and 3 different days
                    if unique_metrics >= 2 and unique_days >= 3:
                        logger.info(f"Found sufficient data using {days} days ({len(health_data)} records, {unique_metrics} metrics, {unique_days} days)")
                        return health_data, days
                
            # If still no good data, return whatever we can get from maximum period
            health_data = self._get_user_health_data(user_id, 365, db)
            return health_data, 365
            
        except Exception as e:
            logger.error(f"Error in optimal data range detection: {e}")
            return pd.DataFrame(), requested_days
    
    def _generate_correlation_insights(
        self, 
        user_id: int, 
        health_data: pd.DataFrame
    ) -> List[HealthInsight]:
        """Generate insights based on correlations between metrics"""
        insights = []
        
        try:
            correlations = self.correlation_analyzer.find_significant_correlations(health_data)
            
            for correlation in correlations:
                if correlation['strength'] > 0.6:  # Strong correlation
                    insight = HealthInsight(
                        id=f"corr_{user_id}_{correlation['metric1']}_{correlation['metric2']}",
                        user_id=user_id,
                        insight_type=InsightType.CORRELATION,
                        priority=InsightPriority.MEDIUM,
                        title=f"Strong Connection: {correlation['metric1']} & {correlation['metric2']}",
                        description=f"Your {correlation['metric1']} shows a {correlation['strength']:.0%} correlation with {correlation['metric2']}. {correlation['interpretation']}",
                        data_sources=list(health_data['source_type'].unique()),
                        metrics_involved=[correlation['metric1'], correlation['metric2']],
                        confidence_score=float(correlation['strength']),
                        actionable_recommendations=correlation.get('recommendations', []),
                        supporting_data=clean_numpy_data(correlation),
                        created_at=datetime.utcnow()
                    )
                    insights.append(insight)
        
        except Exception as e:
            logger.error(f"Error generating correlation insights: {str(e)}")
        
        return insights
    
    def _generate_trend_insights(
        self, 
        user_id: int, 
        health_data: pd.DataFrame
    ) -> List[HealthInsight]:
        """Generate insights based on trends in health metrics"""
        insights = []
        
        try:
            trends = self.pattern_recognizer.analyze_trends(health_data)
            
            for trend in trends:
                priority = InsightPriority.HIGH if abs(trend['slope']) > 0.5 else InsightPriority.MEDIUM
                
                direction = "improving" if trend['slope'] > 0 else "declining"
                
                insight = HealthInsight(
                    id=f"trend_{user_id}_{trend['metric']}",
                    user_id=user_id,
                    insight_type=InsightType.TREND,
                    priority=priority,
                    title=f"{trend['metric'].title()} is {direction}",
                    description=f"Your {trend['metric']} has been {direction} by {abs(trend['slope']):.1f}% over the past {trend['period']} days.",
                    data_sources=list(health_data['source_type'].unique()),
                    metrics_involved=[trend['metric']],
                    confidence_score=float(trend['confidence']),
                    actionable_recommendations=trend.get('recommendations', []),
                    supporting_data=clean_numpy_data(trend),
                    created_at=datetime.utcnow()
                )
                insights.append(insight)
        
        except Exception as e:
            logger.error(f"Error generating trend insights: {str(e)}")
        
        return insights
    
    def _generate_anomaly_insights(
        self, 
        user_id: int, 
        health_data: pd.DataFrame
    ) -> List[HealthInsight]:
        """Generate insights based on anomalies in health data"""
        insights = []
        
        try:
            anomalies = self.anomaly_detector.detect_anomalies(health_data)
            
            for anomaly in anomalies:
                priority = InsightPriority.CRITICAL if anomaly['severity'] > 0.8 else InsightPriority.HIGH
                
                insight = HealthInsight(
                    id=f"anomaly_{user_id}_{anomaly['metric']}_{anomaly['date']}",
                    user_id=user_id,
                    insight_type=InsightType.ANOMALY,
                    priority=priority,
                    title=f"Unusual {anomaly['metric'].title()} Pattern Detected",
                    description=f"Your {anomaly['metric']} on {anomaly['date']} was {anomaly['deviation']:.1f}% different from your typical pattern.",
                    data_sources=list(health_data['source_type'].unique()),
                    metrics_involved=[anomaly['metric']],
                    confidence_score=float(anomaly['confidence']),
                    actionable_recommendations=anomaly.get('recommendations', []),
                    supporting_data=clean_numpy_data(anomaly),
                    created_at=datetime.utcnow()
                )
                insights.append(insight)
        
        except Exception as e:
            logger.error(f"Error generating anomaly insights: {str(e)}")
        
        return insights
    
    def _generate_pattern_insights(
        self, 
        user_id: int, 
        health_data: pd.DataFrame
    ) -> List[HealthInsight]:
        """Generate insights based on patterns in health data"""
        insights = []
        
        try:
            patterns = self.pattern_recognizer.identify_patterns(health_data)
            
            for pattern in patterns:
                insight = HealthInsight(
                    id=f"pattern_{user_id}_{pattern['type']}",
                    user_id=user_id,
                    insight_type=InsightType.PATTERN,
                    priority=InsightPriority.MEDIUM,
                    title=f"{pattern['type'].title()} Pattern Identified",
                    description=pattern['description'],
                    data_sources=list(health_data['source_type'].unique()),
                    metrics_involved=pattern['metrics'],
                    confidence_score=float(pattern['confidence']),
                    actionable_recommendations=pattern.get('recommendations', []),
                    supporting_data=clean_numpy_data(pattern),
                    created_at=datetime.utcnow()
                )
                insights.append(insight)
        
        except Exception as e:
            logger.error(f"Error generating pattern insights: {str(e)}")
        
        return insights
    
    def _generate_achievement_insights(
        self, 
        user_id: int, 
        health_data: pd.DataFrame
    ) -> List[HealthInsight]:
        """Generate insights based on achievements and milestones"""
        insights = []
        
        try:
            # Check for various achievements
            achievements = self._check_achievements(health_data)
            
            for achievement in achievements:
                insight = HealthInsight(
                    id=f"achievement_{user_id}_{achievement['type']}",
                    user_id=user_id,
                    insight_type=InsightType.ACHIEVEMENT,
                    priority=InsightPriority.MEDIUM,
                    title=f"ðŸŽ‰ {achievement['title']}",
                    description=achievement['description'],
                    data_sources=list(health_data['source_type'].unique()),
                    metrics_involved=achievement['metrics'],
                    confidence_score=1.0,
                    actionable_recommendations=achievement.get('recommendations', []),
                    supporting_data=clean_numpy_data(achievement),
                    created_at=datetime.utcnow()
                )
                insights.append(insight)
        
        except Exception as e:
            logger.error(f"Error generating achievement insights: {str(e)}")
        
        return insights
    
    def _generate_recommendation_insights(
        self, 
        user_id: int, 
        health_data: pd.DataFrame
    ) -> List[HealthInsight]:
        """Generate personalized recommendations"""
        insights = []
        
        try:
            recommendations = self.recommendation_engine.generate_recommendations(health_data)
            
            for rec in recommendations:
                insight = HealthInsight(
                    id=f"rec_{user_id}_{rec['category']}",
                    user_id=user_id,
                    insight_type=InsightType.RECOMMENDATION,
                    priority=InsightPriority.MEDIUM,
                    title=rec['title'],
                    description=rec['description'],
                    data_sources=list(health_data['source_type'].unique()),
                    metrics_involved=rec['metrics'],
                    confidence_score=float(rec['confidence']),
                    actionable_recommendations=rec['actions'],
                    supporting_data=clean_numpy_data(rec),
                    created_at=datetime.utcnow()
                )
                insights.append(insight)
        
        except Exception as e:
            logger.error(f"Error generating recommendation insights: {str(e)}")
        
        return insights
    
    def _calculate_activity_score(self, health_data: pd.DataFrame, days_analyzed: int = 30) -> float:
        """Calculate activity score with temporal pattern analysis for time-period sensitivity"""
        try:
            activity_metrics = health_data[
                health_data['metric_type'].isin(['activity_steps', 'activity_workouts', 'activity_active_energy'])
            ]
            
            if activity_metrics.empty:
                return 50.0  # Default score
            
            # Calculate based on temporal patterns and recency weighting
            steps_data = activity_metrics[activity_metrics['metric_type'] == 'activity_steps']
            if not steps_data.empty:
                # Sort by date for temporal analysis
                steps_data = steps_data.sort_values('recorded_at')
                steps_values = steps_data['value'].values
                
                # Calculate recency-weighted average based on time period
                if days_analyzed <= 7:
                    # Recent week - heavy weight on recent days
                    weights = np.linspace(0.5, 2.0, len(steps_values))
                    target_steps = 10000
                    recency_factor = 1.3
                elif days_analyzed <= 30:
                    # Month view - moderate recency weighting  
                    weights = np.linspace(0.7, 1.5, len(steps_values))
                    target_steps = 9500
                    recency_factor = 1.1
                else:
                    # Longer periods - focus on overall consistency
                    weights = np.linspace(0.8, 1.2, len(steps_values))
                    target_steps = 9000
                    recency_factor = 0.95
                
                # Calculate weighted average
                if len(weights) == len(steps_values):
                    weighted_avg_steps = np.average(steps_values, weights=weights)
                else:
                    weighted_avg_steps = np.mean(steps_values)
                
                # Calculate temporal variability score
                if len(steps_values) > 1:
                    std_steps = np.std(steps_values)
                    cv = std_steps / max(weighted_avg_steps, 1)  # Coefficient of variation
                    variability_penalty = min(10, cv * 20)  # Penalize high variability
                else:
                    variability_penalty = 0
                
                # Base score with temporal adjustments
                base_score = (weighted_avg_steps / target_steps) * 100
                temporal_score = base_score * recency_factor - variability_penalty
                steps_score = max(30, min(100, temporal_score))
            else:
                steps_score = 50
            
            # Add workout frequency bonus with time-period scaling
            workouts_data = activity_metrics[activity_metrics['metric_type'] == 'activity_workouts']
            workout_frequency = len(workouts_data) / max(1, days_analyzed) * 7  # Workouts per week
            workout_bonus = min(15, workout_frequency * 4)  # Up to 15 points for workouts
            
            return min(100, steps_score + workout_bonus)
            
        except Exception:
            return 50.0
    
    def _calculate_sleep_score(self, health_data: pd.DataFrame, days_analyzed: int = 30) -> float:
        """Calculate sleep score with enhanced sensitivity and time-period awareness"""
        try:
            sleep_data = health_data[health_data['metric_type'] == 'sleep_duration']
            
            if sleep_data.empty:
                return 50.0
            
            # Sort by date for temporal analysis
            sleep_data = sleep_data.sort_values('recorded_at')
            avg_sleep = sleep_data['value'].mean()
            std_sleep = sleep_data['value'].std()
            
            # Check for NaN values
            if pd.isna(avg_sleep):
                return 50.0
            
            # Enhanced sleep duration scoring with finer gradations
            if 7.5 <= avg_sleep <= 8.5:  # Ideal range (7.5-8.5 hours)
                duration_score = 100.0
            elif 7.0 <= avg_sleep < 7.5:  # Good but slightly short
                duration_score = 85.0 + ((avg_sleep - 7.0) / 0.5) * 15.0  # 85-100
            elif 8.5 < avg_sleep <= 9.0:  # Good but slightly long
                duration_score = 85.0 + ((9.0 - avg_sleep) / 0.5) * 15.0  # 85-100
            elif 6.5 <= avg_sleep < 7.0:  # Moderately short
                duration_score = 65.0 + ((avg_sleep - 6.5) / 0.5) * 20.0  # 65-85
            elif 9.0 < avg_sleep <= 9.5:  # Moderately long
                duration_score = 65.0 + ((9.5 - avg_sleep) / 0.5) * 20.0  # 65-85
            elif 6.0 <= avg_sleep < 6.5:  # Short sleep
                duration_score = 40.0 + ((avg_sleep - 6.0) / 0.5) * 25.0  # 40-65
            elif 9.5 < avg_sleep <= 10.0:  # Long sleep
                duration_score = 40.0 + ((10.0 - avg_sleep) / 0.5) * 25.0  # 40-65
            elif avg_sleep < 6.0:  # Very short
                duration_score = max(20.0, 40.0 * (avg_sleep / 6.0))  # 20-40
            else:  # avg_sleep > 10.0 - Very long
                duration_score = max(20.0, 40.0 - ((avg_sleep - 10.0) * 5.0))  # Diminishing
            
            # Enhanced consistency scoring with time-period sensitivity
            if pd.isna(std_sleep) or len(sleep_data) == 1:
                consistency_score = 100.0  # Perfect consistency for single data point
            else:
                # Calculate coefficient of variation (more meaningful than just std dev)
                cv = std_sleep / avg_sleep if avg_sleep > 0 else 1.0
                
                # Base consistency score (lower CV = higher consistency)
                base_consistency = max(0, 100 - (cv * 100))
                
                # Time-period specific adjustments
                if days_analyzed <= 7:
                    # Weekly view - stricter consistency requirements (daily variations matter more)
                    consistency_score = base_consistency * 1.2  # Amplify consistency importance
                elif days_analyzed <= 30:
                    # Monthly view - moderate consistency requirements
                    consistency_score = base_consistency * 1.1
                else:
                    # Longer periods - slight penalty for high variation
                    consistency_score = base_consistency * 0.95
                
                # Ensure within bounds
                consistency_score = min(100, max(10, consistency_score))
            
            # Temporal pattern analysis for additional scoring
            temporal_bonus = 0.0
            if len(sleep_data) >= 7:  # Need sufficient data for temporal analysis
                sleep_data['date'] = pd.to_datetime(sleep_data['recorded_at']).dt.date
                daily_sleep = sleep_data.groupby('date')['value'].mean()
                
                if days_analyzed <= 7:
                    # Week view - reward improving recent sleep
                    if len(daily_sleep) >= 4:
                        recent_sleep = daily_sleep.tail(3).mean()
                        older_sleep = daily_sleep.head(3).mean()
                        if recent_sleep > older_sleep:
                            temporal_bonus = min(10, (recent_sleep - older_sleep) * 5)
                elif days_analyzed <= 30:
                    # Month view - reward sustained good sleep
                    good_sleep_days = len(daily_sleep[(daily_sleep >= 7.0) & (daily_sleep <= 9.0)])
                    good_sleep_ratio = good_sleep_days / len(daily_sleep)
                    if good_sleep_ratio > 0.7:
                        temporal_bonus = (good_sleep_ratio - 0.7) * 20  # Up to 6 points bonus
                else:
                    # Long-term: Reward long-term stability with good average
                    if avg_sleep >= 7.0 and avg_sleep <= 9.0 and cv < 0.15:  # Stable and good
                        temporal_bonus = 8.0
            
            # Weighted final score with period-sensitive weighting
            if days_analyzed <= 7:
                # Week: Emphasize recent consistency and patterns
                final_score = (duration_score * 0.6 + consistency_score * 0.35 + temporal_bonus * 0.05)
            elif days_analyzed <= 30:
                # Month: Balance all factors
                final_score = (duration_score * 0.65 + consistency_score * 0.3 + temporal_bonus * 0.05)
            else:
                # Long-term: Emphasize overall duration quality
                final_score = (duration_score * 0.7 + consistency_score * 0.25 + temporal_bonus * 0.05)
            
            # Ensure final score is valid and within bounds
            if pd.isna(final_score):
                return 50.0
            
            return min(100.0, max(20.0, float(final_score)))
            
        except Exception as e:
            logger.error(f"Error calculating sleep score: {e}")
            return 50.0
    
    def _calculate_nutrition_score(self, health_data: pd.DataFrame, days_analyzed: int = 30) -> float:
        """Calculate nutrition score with enhanced analysis of intake quality, balance, and time-period sensitivity"""
        try:
            nutrition_data = health_data[
                health_data['metric_type'].isin(['nutrition_calories', 'nutrition_protein', 'nutrition_carbs', 'nutrition_fat'])
            ]
            
            if nutrition_data.empty:
                return 50.0
            
            # Analyze calorie intake with time-period specific targets
            calories_data = nutrition_data[nutrition_data['metric_type'] == 'nutrition_calories']
            if calories_data.empty:
                return 50.0
            
            # Sort by date for temporal analysis
            calories_data = calories_data.sort_values('recorded_at')
            calories_values = calories_data['value'].values
            avg_calories = np.mean(calories_values)
            std_calories = np.std(calories_values)
            
            # Time-period specific calorie adequacy scoring
            if days_analyzed <= 7:
                # Week view - focus on recent intake patterns
                target_calories = 2000  # Base target
                adequacy_tolerance = 300  # Â±300 calories for optimal
                consistency_weight = 0.4
            elif days_analyzed <= 30:
                # Month view - balanced long-term intake assessment
                target_calories = 2000
                adequacy_tolerance = 250  # Stricter tolerance for month
                consistency_weight = 0.35
            else:
                # Long-term view - emphasize sustained healthy patterns
                target_calories = 2000
                adequacy_tolerance = 200  # Strictest tolerance for long-term
                consistency_weight = 0.3
            
            # Calorie adequacy score (50% of total nutrition score)
            calorie_deficit = abs(avg_calories - target_calories)
            if calorie_deficit <= adequacy_tolerance:
                adequacy_score = 100.0  # Optimal range
            elif calorie_deficit <= adequacy_tolerance * 2:
                # Moderate deviation - gradual penalty
                adequacy_score = 85.0 - ((calorie_deficit - adequacy_tolerance) / adequacy_tolerance * 25.0)
            elif calorie_deficit <= adequacy_tolerance * 3:
                # Significant deviation
                adequacy_score = 60.0 - ((calorie_deficit - adequacy_tolerance * 2) / adequacy_tolerance * 20.0)
            else:
                # Major deviation
                adequacy_score = max(30.0, 40.0 - (calorie_deficit - adequacy_tolerance * 3) / 100)
            
            # Consistency score based on coefficient of variation
            if len(calories_values) == 1:
                consistency_score = 100.0
            else:
                cv = std_calories / max(avg_calories, 1)  # Coefficient of variation
                
                # Time-period specific consistency analysis
                if days_analyzed <= 7:
                    # Week: More tolerance for day-to-day variation
                    base_consistency = max(0, 100 - (cv * 80))
                elif days_analyzed <= 30:
                    # Month: Moderate consistency expectations
                    base_consistency = max(0, 100 - (cv * 100))
                else:
                    # Long-term: Higher consistency expectations
                    base_consistency = max(0, 100 - (cv * 120))
                
                consistency_score = min(100, max(20, base_consistency))
            
            # Macro balance analysis (when available)
            protein_data = nutrition_data[nutrition_data['metric_type'] == 'nutrition_protein']
            carbs_data = nutrition_data[nutrition_data['metric_type'] == 'nutrition_carbs']
            fat_data = nutrition_data[nutrition_data['metric_type'] == 'nutrition_fat']
            
            balance_score = 70.0  # Default when macro data not available
            if not protein_data.empty and not carbs_data.empty:
                avg_protein = protein_data['value'].mean()
                avg_carbs = carbs_data['value'].mean()
                
                # Calculate protein percentage of total calories (4 cal/g protein, 4 cal/g carbs)
                protein_calories = avg_protein * 4
                carbs_calories = avg_carbs * 4
                total_macro_calories = protein_calories + carbs_calories
                
                if total_macro_calories > 0:
                    protein_percent = (protein_calories / total_macro_calories) * 100
                    
                    # Optimal protein range: 15-25% of calories
                    if 15 <= protein_percent <= 25:
                        balance_score = 100.0
                    elif 10 <= protein_percent < 15 or 25 < protein_percent <= 30:
                        balance_score = 85.0
                    elif 8 <= protein_percent < 10 or 30 < protein_percent <= 35:
                        balance_score = 70.0
                    else:
                        balance_score = 50.0
            
            # Temporal pattern analysis
            temporal_bonus = 0.0
            if len(calories_values) >= 7:
                calories_data['date'] = pd.to_datetime(calories_data['recorded_at']).dt.date
                daily_calories = calories_data.groupby('date')['value'].mean()
                
                if days_analyzed <= 7:
                    # Week: Reward improving recent intake
                    if len(daily_calories) >= 4:
                        recent_avg = daily_calories.tail(3).mean()
                        if 1800 <= recent_avg <= 2200:  # Recent intake in healthy range
                            temporal_bonus = 5.0
                elif days_analyzed <= 30:
                    # Month: Reward sustained healthy intake
                    healthy_days = len(daily_calories[(daily_calories >= 1800) & (daily_calories <= 2200)])
                    healthy_ratio = healthy_days / len(daily_calories)
                    if healthy_ratio > 0.7:
                        temporal_bonus = (healthy_ratio - 0.7) * 15  # Up to 4.5 points
                else:
                    # Long-term: Reward stability with good averages
                    if 1900 <= avg_calories <= 2100 and cv < 0.25:  # Stable and healthy
                        temporal_bonus = 8.0
            
            # Weighted final score with time-period specific emphasis
            if days_analyzed <= 7:
                # Week: Balance adequacy and recent patterns
                final_score = (adequacy_score * 0.5 + consistency_score * consistency_weight + 
                              balance_score * 0.2 + temporal_bonus)
            elif days_analyzed <= 30:
                # Month: Emphasize balance and adequacy
                final_score = (adequacy_score * 0.45 + consistency_score * consistency_weight + 
                              balance_score * 0.25 + temporal_bonus)
            else:
                # Long-term: Focus on sustained quality and balance
                final_score = (adequacy_score * 0.4 + consistency_score * consistency_weight + 
                              balance_score * 0.3 + temporal_bonus)
            
            # Ensure final score is valid and within bounds
            if pd.isna(final_score):
                return 50.0
            
            return min(100.0, max(30.0, float(final_score)))
            
        except Exception as e:
            logger.error(f"Error calculating nutrition score: {e}")
            return 50.0
    
    def _calculate_heart_health_score(self, health_data: pd.DataFrame, days_analyzed: int = 30) -> float:
        """Calculate heart health score with enhanced analysis of HR patterns, variability, and time-period sensitivity"""
        try:
            heart_data = health_data[
                health_data['metric_type'].isin(['heart_rate', 'resting_heart_rate', 'heart_rate_variability'])
            ]
            
            if heart_data.empty:
                return 50.0
            
            # Prioritize resting heart rate, fallback to general heart rate
            resting_hr_data = heart_data[heart_data['metric_type'] == 'resting_heart_rate']
            if resting_hr_data.empty:
                # Fallback to general heart rate if resting HR not available
                resting_hr_data = heart_data[heart_data['metric_type'] == 'heart_rate']
                if resting_hr_data.empty:
                    return 50.0  # No heart rate data available
            
            # Sort by date for temporal analysis
            resting_hr_data = resting_hr_data.sort_values('recorded_at')
            hr_values = resting_hr_data['value'].values
            avg_resting_hr = np.mean(hr_values)
            std_resting_hr = np.std(hr_values)
            
            # Enhanced resting HR scoring with fine gradations
            if 50 <= avg_resting_hr <= 60:  # Excellent (athlete-level)
                hr_score = 100.0
            elif 60 < avg_resting_hr <= 70:  # Very good
                hr_score = 90.0 + ((70 - avg_resting_hr) / 10) * 10.0  # 90-100
            elif 70 < avg_resting_hr <= 80:  # Good/normal
                hr_score = 75.0 + ((80 - avg_resting_hr) / 10) * 15.0  # 75-90
            elif 80 < avg_resting_hr <= 90:  # Slightly elevated
                hr_score = 60.0 + ((90 - avg_resting_hr) / 10) * 15.0  # 60-75
            elif 90 < avg_resting_hr <= 100:  # Elevated
                hr_score = 45.0 + ((100 - avg_resting_hr) / 10) * 15.0  # 45-60
            elif avg_resting_hr < 50:  # Bradycardia range (could be concerning or excellent)
                # Context-dependent: if stable, likely athletic; if inconsistent, concerning
                if len(hr_values) > 1 and std_resting_hr < 5:
                    hr_score = 95.0  # Likely athletic
                else:
                    hr_score = 70.0  # Potential concern, needs evaluation
            else:  # > 100 BPM - concerning
                hr_score = max(30.0, 45.0 - ((avg_resting_hr - 100) * 2.0))
            
            # HR consistency analysis with time-period sensitivity
            if len(hr_values) == 1:
                consistency_score = 90.0  # Single measurement, assume decent consistency
            else:
                cv = std_resting_hr / max(avg_resting_hr, 1)  # Coefficient of variation
                
                # Time-period specific consistency analysis
                if days_analyzed <= 7:
                    # Week: More tolerance for day-to-day HR variation
                    base_consistency = max(0, 100 - (cv * 200))
                elif days_analyzed <= 30:
                    # Month: Moderate consistency expectations
                    base_consistency = max(0, 100 - (cv * 250))
                else:
                    # Long-term: Higher consistency expectations
                    base_consistency = max(0, 100 - (cv * 300))
                
                consistency_score = min(100, max(40, base_consistency))
            
            # Heart Rate Variability analysis (when available)
            hrv_data = heart_data[heart_data['metric_type'] == 'heart_rate_variability']
            hrv_score = 70.0  # Default when HRV not available
            
            if not hrv_data.empty:
                avg_hrv = hrv_data['value'].mean()
                
                # HRV scoring (higher is generally better for most HRV metrics)
                if avg_hrv >= 50:  # Excellent HRV
                    hrv_score = 100.0
                elif avg_hrv >= 40:  # Very good
                    hrv_score = 85.0 + ((avg_hrv - 40) / 10) * 15.0  # 85-100
                elif avg_hrv >= 30:  # Good
                    hrv_score = 70.0 + ((avg_hrv - 30) / 10) * 15.0  # 70-85
                elif avg_hrv >= 20:  # Below average
                    hrv_score = 55.0 + ((avg_hrv - 20) / 10) * 15.0  # 55-70
                else:  # Poor HRV
                    hrv_score = max(40.0, 55.0 * (avg_hrv / 20))
            
            # Temporal trend analysis
            temporal_bonus = 0.0
            if len(hr_values) >= 7:
                resting_hr_data['date'] = pd.to_datetime(resting_hr_data['recorded_at']).dt.date
                daily_hr = resting_hr_data.groupby('date')['value'].mean()
                
                if days_analyzed <= 7:
                    # Week: Reward improving recent HR
                    if len(daily_hr) >= 4:
                        recent_avg = daily_hr.tail(3).mean()
                        older_avg = daily_hr.head(3).mean()
                        if recent_avg < older_avg:  # Lower is better for resting HR
                            improvement = older_avg - recent_avg
                            temporal_bonus = min(8.0, improvement * 2.0)
                elif days_analyzed <= 30:
                    # Month: Reward sustained healthy HR patterns
                    healthy_hr_days = len(daily_hr[(daily_hr >= 50) & (daily_hr <= 80)])
                    healthy_ratio = healthy_hr_days / len(daily_hr)
                    if healthy_ratio > 0.7:
                        temporal_bonus = (healthy_ratio - 0.7) * 20  # Up to 6 points
                else:
                    # Long-term: Reward stability in healthy range
                    if 55 <= avg_resting_hr <= 75 and cv < 0.1:  # Stable and healthy
                        temporal_bonus = 10.0
            
            # Age-based adjustments (if we had age data, we'd use it here)
            # For now, we'll use the standard adult ranges
            
            # Weighted final score with time-period specific emphasis
            if days_analyzed <= 7:
                # Week: Emphasize recent patterns and consistency
                final_score = (hr_score * 0.6 + consistency_score * 0.25 + 
                              hrv_score * 0.15 + temporal_bonus)
            elif days_analyzed <= 30:
                # Month: Balance all factors
                final_score = (hr_score * 0.55 + consistency_score * 0.25 + 
                              hrv_score * 0.2 + temporal_bonus)
            else:
                # Long-term: Emphasize overall HR health and HRV
                final_score = (hr_score * 0.5 + consistency_score * 0.2 + 
                              hrv_score * 0.3 + temporal_bonus)
            
            # Ensure final score is valid and within bounds
            if pd.isna(final_score):
                return 50.0
            
            return min(100.0, max(30.0, float(final_score)))
            
        except Exception as e:
            logger.error(f"Error calculating heart health score: {e}")
            return 50.0
    
    def _calculate_consistency_score(self, health_data: pd.DataFrame, days_analyzed: int = 30) -> float:
        """Calculate consistency score with temporal pattern analysis for better time-period sensitivity"""
        try:
            # Count unique days with data
            health_data['date'] = pd.to_datetime(health_data['recorded_at']).dt.date
            unique_days = health_data['date'].nunique()
            
            # Calculate consistency based on data frequency and temporal patterns
            max_timestamp = health_data['recorded_at'].max()
            min_timestamp = health_data['recorded_at'].min()
            
            # Check for NaN timestamps
            if pd.isna(max_timestamp) or pd.isna(min_timestamp):
                return 50.0
            
            total_days = (max_timestamp - min_timestamp).days + 1
            
            # Avoid division by zero
            if total_days <= 0:
                return 100.0  # Single day = perfect consistency
            
            # Calculate base consistency
            base_consistency = (unique_days / min(days_analyzed, total_days)) * 100
            
            # Add temporal pattern analysis
            daily_data_counts = health_data.groupby('date').size()
            
            if len(daily_data_counts) > 1:
                # Analyze data frequency patterns
                if days_analyzed <= 7:
                    # Recent week - focus on recent consistency
                    recent_data = daily_data_counts.tail(min(7, len(daily_data_counts)))
                    recent_consistency = len(recent_data) / min(7, days_analyzed) * 100
                    temporal_bonus = 20 if recent_consistency > 80 else 10
                    final_score = base_consistency * 1.4 + temporal_bonus
                elif days_analyzed <= 30:
                    # Month view - balance recent and overall
                    recent_data = daily_data_counts.tail(min(14, len(daily_data_counts)))
                    recent_consistency = len(recent_data) / min(14, days_analyzed) * 100
                    overall_variability = daily_data_counts.std() / max(daily_data_counts.mean(), 1)
                    variability_penalty = min(15, overall_variability * 10)
                    final_score = base_consistency * 1.2 - variability_penalty
                else:
                    # Longer periods - reward sustained consistency
                    data_gaps = self._calculate_data_gaps(daily_data_counts)
                    gap_penalty = min(20, data_gaps * 5)
                    sustained_bonus = 15 if base_consistency > 70 else 5
                    final_score = base_consistency * 0.9 + sustained_bonus - gap_penalty
            else:
                final_score = base_consistency
            
            # Ensure result is not NaN and within bounds
            if pd.isna(final_score):
                return 50.0
            
            return min(100, max(30, final_score))
            
        except Exception:
            return 50.0
    
    def _calculate_data_gaps(self, daily_data_counts):
        """Calculate data gap penalties for consistency scoring"""
        try:
            dates = pd.to_datetime(daily_data_counts.index)
            if len(dates) <= 1:
                return 0
            
            # Find gaps larger than 2 days
            date_diffs = dates.diff().dt.days.fillna(1)
            large_gaps = date_diffs[date_diffs > 2]
            return len(large_gaps)
        except:
            return 0
    
    def _calculate_trend_score(self, health_data: pd.DataFrame, days_analyzed: int = 30) -> float:
        """Calculate trend score based on improvement over time with period-sensitive analysis"""
        try:
            # Analyze trends in key metrics with period-appropriate windows
            key_metrics = ['activity_steps', 'sleep_duration', 'heart_rate_resting']
            trend_scores = []
            
            # Adjust comparison windows based on analysis period
            if days_analyzed <= 7:
                # For week view, compare last 3 days vs first 3 days
                recent_window = 3
                older_window = 3
                min_data_points = 5
            elif days_analyzed <= 30:
                # For month view, compare last week vs first week
                recent_window = 7
                older_window = 7
                min_data_points = 10
            else:
                # For longer periods, compare last 2 weeks vs first 2 weeks
                recent_window = 14
                older_window = 14
                min_data_points = 20
            
            for metric in key_metrics:
                metric_data = health_data[health_data['metric_type'] == metric]
                if len(metric_data) >= min_data_points:  # Adjusted minimum data requirement
                    # Time-sensitive trend calculation
                    metric_data = metric_data.sort_values('recorded_at')
                    recent_avg = metric_data.tail(recent_window)['value'].mean()
                    older_avg = metric_data.head(older_window)['value'].mean()
                    
                    # Check for NaN values before calculation
                    if pd.isna(recent_avg) or pd.isna(older_avg) or older_avg == 0:
                        continue
                    
                    if metric == 'heart_rate_resting':
                        # Lower is better for resting heart rate
                        trend = (older_avg - recent_avg) / older_avg * 100
                    else:
                        # Higher is better for steps and sleep
                        trend = (recent_avg - older_avg) / older_avg * 100
                    
                    # Ensure trend is not NaN or infinite
                    if not pd.isna(trend) and np.isfinite(trend):
                        # Scale trend impact based on time period
                        if days_analyzed <= 7:
                            # Short-term trends are more volatile, moderate the impact
                            trend_impact = trend * 3
                        elif days_analyzed <= 30:
                            # Standard trend impact
                            trend_impact = trend * 5
                        else:
                            # Long-term trends are more meaningful, amplify slightly
                            trend_impact = trend * 6
                        
                        trend_scores.append(max(0, min(100, 50 + trend_impact)))
            
            # Use np.nanmean to handle any remaining NaN values, with fallback
            if trend_scores:
                result = np.nanmean(trend_scores)
                return float(result) if not pd.isna(result) else 50.0
            else:
                return 50.0
            
        except Exception:
            return 50.0
    
    def _check_achievements(self, health_data: pd.DataFrame) -> List[Dict[str, Any]]:
        """Check for various achievements and milestones"""
        achievements = []
        
        try:
            # Step achievements
            steps_data = health_data[health_data['metric_type'] == 'activity_steps']
            if not steps_data.empty:
                max_steps = steps_data['value'].max()
                if max_steps >= 20000:
                    achievements.append({
                        'type': 'steps_20k',
                        'title': 'Step Champion!',
                        'description': f'You walked {max_steps:,.0f} steps in a single day!',
                        'metrics': ['activity_steps']
                    })
                elif max_steps >= 15000:
                    achievements.append({
                        'type': 'steps_15k',
                        'title': 'Walking Warrior!',
                        'description': f'You achieved {max_steps:,.0f} steps in one day!',
                        'metrics': ['activity_steps']
                    })
            
            # Consistency achievements
            health_data['date'] = pd.to_datetime(health_data['recorded_at']).dt.date
            unique_days = health_data['date'].nunique()
            if unique_days >= 30:
                achievements.append({
                    'type': 'consistency_30',
                    'title': 'Consistency Master!',
                    'description': f'You\'ve tracked your health for {unique_days} days straight!',
                    'metrics': ['all']
                })
            elif unique_days >= 7:
                achievements.append({
                    'type': 'consistency_7',
                    'title': 'Week Warrior!',
                    'description': f'You\'ve maintained {unique_days} days of health tracking!',
                    'metrics': ['all']
                })
        
        except Exception as e:
            logger.error(f"Error checking achievements: {str(e)}")
        
        return achievements
    
    def _identify_available_metrics(self, health_data: pd.DataFrame) -> dict:
        """Identify which metric categories have sufficient data for scoring"""
        available = {}
        
        # Activity metrics
        activity_metrics = health_data[health_data['metric_type'].isin(['activity_steps', 'activity_workouts', 'activity_active_energy'])]
        available['activity'] = not activity_metrics.empty
        
        # Sleep metrics  
        sleep_metrics = health_data[health_data['metric_type'] == 'sleep_duration']
        available['sleep'] = not sleep_metrics.empty
        
        # Nutrition metrics
        nutrition_metrics = health_data[health_data['metric_type'].isin(['nutrition_calories', 'nutrition_protein', 'nutrition_carbs', 'nutrition_fat'])]
        available['nutrition'] = not nutrition_metrics.empty
        
        # Heart health metrics (FIXED: using correct metric name)
        heart_metrics = health_data[health_data['metric_type'].isin(['heart_rate', 'resting_heart_rate', 'heart_rate_variability'])]
        available['heart_health'] = not heart_metrics.empty
        
        return available


# Global instance
health_insights_engine = HealthInsightsEngine() 